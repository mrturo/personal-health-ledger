# Personal Health Ledger - Quick Reference

## ğŸš€ Installation & Setup

```bash
# 1. Clone repository
git clone <your-repo-url>
cd personal-health-ledger

# 2. Run automated setup
./setup.sh

# 3. Configure Google Drive (choose one):

# Option A: OAuth2 (personal use)
cp config/credentials.json.example config/credentials.json
# Edit with your OAuth2 credentials from Google Cloud Console

# Option B: Service Account (automation)
cp config/service_account.json.example config/service_account.json
# Edit with your service account key
# Share Drive folder with service account email

# 4. Customize settings
vim config/config.yaml  # Adjust timezone, folder name, etc.
```

## ğŸ“‹ CLI Commands

```bash
# Activate environment
source venv/bin/activate

# Full pipeline (recommended)
phl all

# Individual steps
phl sync                              # Download from Drive
phl build                             # Parse + consolidate
phl compare                           # CSV vs FIT analysis
phl daily                             # Generate daily averages

# With options
phl sync --folder-id "ABC123"         # Override folder
phl sync --force                      # Re-download all files
phl build --timezone "America/Santiago"  # Override timezone
phl build --tolerance-seconds 120     # Increase timestamp tolerance
phl build --output-format parquet     # Parquet only
phl compare --tolerance-seconds 30    # Stricter matching

# Get help
phl --help
phl sync --help
phl build --help
```

## ğŸ“Š Output Files

```
output/
â”œâ”€â”€ weight_consolidated.csv           # Main dataset (JSON-serialized lineage)
â”œâ”€â”€ weight_consolidated.parquet       # Main dataset (native types)
â”œâ”€â”€ weight_daily.csv                  # Daily averages (generated by phl daily)
â”œâ”€â”€ conflicts.csv                     # Records with CSVâ‰ FIT conflicts
â”œâ”€â”€ comparison_summary.json           # Quality metrics per file pair
â”œâ”€â”€ ingestion_log.jsonl              # Processing events
â””â”€â”€ app.log                          # Application logs
```

## ğŸ” Understanding Lineage Fields

Every consolidated record includes:

```python
{
  "record_id": "abc123...",                    # Deterministic hash
  "timestamp": "2024-01-15T10:30:00-06:00",   # Measurement time
  "weight_kg": 75.5,                          # Primary measurement
  
  # Lineage tracking
  "source_files": ["Peso 1-2024.csv", "Peso 1-2024.fit"],
  "source_types": ["csv", "fit"],
  "drive_file_ids": ["file_id_1", "file_id_2"],
  "ingestion_timestamp": "2024-01-15T20:00:00Z",
  
  # Field-level provenance
  "field_sources": {
    "weight_kg": "merged",       # Same in CSV & FIT
    "body_fat_pct": "conflict"   # Different in CSV vs FIT
  },
  
  # Conflicts
  "conflicting_fields": ["body_fat_pct"],
  "body_fat_pct": 18.2,           # Resolved value (or CSV by default)
  "body_fat_pct_csv": 18.2,       # Original CSV value
  "body_fat_pct_fit": 18.5        # Original FIT value
}
```

## âš™ï¸ Configuration Quick Reference

### Key settings in `config/config.yaml`:

```yaml
# Drive authentication
drive:
  auth_method: "oauth2"  # or "service_account"
  folder_name: "Health Sync Weight"

# Processing
processing:
  timezone: "America/Santiago"
  timestamp_tolerance_seconds: 60      # For CSV-FIT matching
  numeric_tolerance: 0.001             # For conflict detection
  
  conflict_resolution:
    default_preference: null           # null, "csv", or "fit"
    field_preferences:
      weight_kg: "csv"                 # Per-field override

# Output
output:
  formats: ["csv", "parquet"]
```

## ğŸ§ª Testing & Quality

```bash
# Run all tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=src/personal_health_ledger --cov-report=term

# Type checking
mypy src/
pyright src/

# Linting
ruff check src/ tests/
ruff check src/ tests/ --fix  # Auto-fix
```

## ğŸ› Troubleshooting

### "Folder not found"
```bash
# Find folder ID in Drive URL: https://drive.google.com/drive/folders/<FOLDER_ID>
phl sync --folder-id "<FOLDER_ID>"
```

### "Authentication failed"
```bash
# OAuth2: Delete token and re-authenticate
rm config/token.json
phl sync

# Service Account: Check folder sharing
# Ensure service account email has Viewer access
```

### "No records parsed"
```bash
# Check downloaded files
ls -la data/raw/

# Check logs
tail -f output/app.log

# Verify file formats
file data/raw/*
```

### "Type check errors"
```bash
# Update dependencies
pip install -U mypy pyright

# Check specific file
mypy src/personal_health_ledger/services/consolidation.py
```

## ğŸ“– Code Examples

### Accessing consolidated data in Python

```python
import pandas as pd
import json

# Load CSV
df = pd.read_csv("output/weight_consolidated.csv")

# Parse JSON fields
df["source_files"] = df["source_files"].apply(json.loads)
df["source_types"] = df["source_types"].apply(json.loads)

# Filter conflicts
conflicts = df[df["conflicting_fields"] != "[]"]

# Load Parquet (native types)
df_parquet = pd.read_parquet("output/weight_consolidated.parquet")
# Lists/dicts already native types
```

### Analyzing comparison results

```python
import json

with open("output/comparison_summary.json") as f:
    summary = json.load(f)

for pair in summary["pairs"]:
    print(f"{pair['csv_file_name']}:")
    print(f"  CSV-only: {pair['csv_only_count']}")
    print(f"  FIT-only: {pair['fit_only_count']}")
    print(f"  Both: {pair['both_count']}")
    print(f"  Weight MAE: {pair['weight_mae']} kg")
```

## ğŸ” Security Best Practices

- âœ… Never commit `credentials.json` or `service_account.json`
- âœ… Add sensitive files to `.gitignore` (already done)
- âœ… Use read-only Drive scopes
- âœ… Rotate service account keys periodically
- âœ… Keep `token.json` secure (contains access tokens)

## ğŸ¯ Common Workflows

### Initial data load
```bash
phl all                      # Downloads + processes everything
```

### Incremental updates
```bash
phl sync                     # Downloads only new/modified files
phl build                    # Reprocesses all data
```

### Quality audit
```bash
phl compare                  # Generates quality report
cat output/comparison_summary.json | jq '.pairs[] | .weight_mae'
```

### Conflict review
```bash
phl build
cat output/conflicts.csv | grep "weight_kg"
```

## ğŸ“š Documentation Links

- **README.md**: Comprehensive guide (setup, usage, features)
- **PROJECT_STRUCTURE.md**: Architecture deep-dive
- **config/README.md**: Credentials setup instructions
- **Code docstrings**: In-code documentation

## ğŸ’¡ Tips

- Use `--help` on any command for detailed options
- Check `output/app.log` for detailed execution logs
- Parquet format preserves native types (better for Python analysis)
- CSV format better for Excel/Google Sheets
- Increase `timestamp_tolerance_seconds` if CSV/FIT timestamps differ by >1 min
- Use `--force` on `phl sync` to re-download all files (useful after config changes)

## ğŸ†˜ Getting Help

1. Check logs: `tail -f output/app.log`
2. Run with verbose output: CLI outputs progress by default
3. Review configuration: `cat config/config.yaml`
4. Validate credentials: Check file exists and has correct JSON format
5. Test Drive connection: `phl sync` (will fail fast if auth broken)

## ğŸ“ Learning the Codebase

Start here to understand the architecture:

1. `src/personal_health_ledger/domain/weight.py` - Core models
2. `src/personal_health_ledger/cli/main.py` - CLI entry points
3. `src/personal_health_ledger/services/consolidation.py` - Core logic
4. `tests/test_consolidation.py` - See how it works via tests

## ğŸš€ Next Steps

After running the pipeline successfully:

1. Explore output files in `output/`
2. Analyze conflicts in `conflicts.csv`
3. Review quality metrics in `comparison_summary.json`
4. Customize `config.yaml` for your needs
5. Consider extending to new domains (sleep, activities)

---

**Version**: 0.1.0  
**Last Updated**: January 2026  
**Questions?** Check README.md or PROJECT_STRUCTURE.md
